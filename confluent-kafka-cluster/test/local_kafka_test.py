#!/usr/bin/env python3
"""
æœ¬åœ°Kafkaæµ‹è¯•è„šæœ¬
ä½¿ç”¨å¤–éƒ¨IPåœ°å€å’ŒmTLSè¿æ¥åˆ°Kafkaé›†ç¾¤
"""

import json
import time
import sys
import base64
from datetime import datetime
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import KafkaError, TopicAlreadyExistsError
import ssl

# Kafkaå¤–éƒ¨IPåœ°å€å’Œç«¯å£æ˜ å°„
KAFKA_BROKERS = [
    '35.197.206.204:9093',   # kafka-0-internal
    '34.147.221.36:9093',    # kafka-1-internal  
    '34.39.39.253:9093'      # kafka-2-internal
]

# è¯ä¹¦æ–‡ä»¶è·¯å¾„
import os
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

CERT_FILES = {
    'ca_cert': os.path.join(PROJECT_ROOT, 'deploy/certs/ca-cert.pem'),
    'client_cert': os.path.join(PROJECT_ROOT, 'deploy/certs/kafka-client-cert.pem'),
    'client_key': os.path.join(PROJECT_ROOT, 'deploy/certs/kafka-client-key.pem')
}

def check_cert_files():
    """æ£€æŸ¥è¯ä¹¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    for name, path in CERT_FILES.items():
        if not os.path.exists(path):
            print(f"âŒ è¯ä¹¦æ–‡ä»¶ä¸å­˜åœ¨: {name} -> {path}")
            return False
        else:
            print(f"âœ… è¯ä¹¦æ–‡ä»¶å­˜åœ¨: {name} -> {path}")
    return True

def create_ssl_context():
    """åˆ›å»ºSSLä¸Šä¸‹æ–‡"""
    # é¦–å…ˆæ£€æŸ¥è¯ä¹¦æ–‡ä»¶
    if not check_cert_files():
        raise FileNotFoundError("è¯ä¹¦æ–‡ä»¶ç¼ºå¤±")
    
    context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
    context.check_hostname = False  # è·³è¿‡ä¸»æœºåéªŒè¯ï¼Œå› ä¸ºä½¿ç”¨IPåœ°å€
    context.verify_mode = ssl.CERT_REQUIRED
    
    # åŠ è½½CAè¯ä¹¦
    context.load_verify_locations(CERT_FILES['ca_cert'])
    
    # åŠ è½½å®¢æˆ·ç«¯è¯ä¹¦å’Œç§é’¥
    context.load_cert_chain(CERT_FILES['client_cert'], CERT_FILES['client_key'])
    
    return context

def create_simple_log_message():
    """åˆ›å»ºç®€å•çš„OTLPæ—¥å¿—æ¶ˆæ¯"""
    timestamp_ns = int(time.time() * 1_000_000_000)
    
    message = {
        "resourceLogs": [{
            "resource": {
                "attributes": [{
                    "key": "service.name",
                    "value": {"stringValue": "local-test-service"}
                }]
            },
            "scopeLogs": [{
                "scope": {"name": "local-test-logger"},
                "logRecords": [{
                    "timeUnixNano": str(timestamp_ns),
                    "severityText": "INFO",
                    "body": {"stringValue": f"æœ¬åœ°æµ‹è¯•æ—¥å¿— - {datetime.now().isoformat()}"}
                }]
            }]
        }]
    }
    
    return json.dumps(message, ensure_ascii=False)

def create_simple_metrics_message():
    """åˆ›å»ºç®€å•çš„OTLPæŒ‡æ ‡æ¶ˆæ¯"""
    timestamp_ns = int(time.time() * 1_000_000_000)
    
    message = {
        "resourceMetrics": [{
            "resource": {
                "attributes": [{
                    "key": "service.name", 
                    "value": {"stringValue": "local-test-service"}
                }]
            },
            "scopeMetrics": [{
                "scope": {"name": "local-test-meter"},
                "metrics": [{
                    "name": "local_test_counter",
                    "sum": {
                        "dataPoints": [{
                            "timeUnixNano": str(timestamp_ns),
                            "asInt": "42"
                        }]
                    }
                }]
            }]
        }]
    }
    
    return json.dumps(message, ensure_ascii=False)

def create_simple_trace_message():
    """åˆ›å»ºç®€å•çš„OTLPè¿½è¸ªæ¶ˆæ¯"""
    timestamp_ns = int(time.time() * 1_000_000_000)
    
    message = {
        "resourceSpans": [{
            "resource": {
                "attributes": [{
                    "key": "service.name",
                    "value": {"stringValue": "local-test-service"}
                }]
            },
            "scopeSpans": [{
                "scope": {"name": "local-test-tracer"},
                "spans": [{
                    "traceId": base64.b64encode(b"1234567890123456").decode(),
                    "spanId": base64.b64encode(b"12345678").decode(),
                    "name": "local-test-span",
                    "startTimeUnixNano": str(timestamp_ns),
                    "endTimeUnixNano": str(timestamp_ns + 1000000)
                }]
            }]
        }]
    }
    
    return json.dumps(message, ensure_ascii=False)

def create_admin_client():
    """åˆ›å»ºKafkaç®¡ç†å®¢æˆ·ç«¯"""
    ssl_context = create_ssl_context()
    
    admin_client = KafkaAdminClient(
        bootstrap_servers=KAFKA_BROKERS,
        security_protocol='SSL',
        ssl_context=ssl_context,
        client_id='local-test-admin',
        api_version=(2, 6, 0),
        request_timeout_ms=30000
    )
    
    return admin_client

def check_and_create_topics():
    """æ£€æŸ¥å¹¶åˆ›å»ºå¿…éœ€çš„topics"""
    print("ğŸ“‹ æ£€æŸ¥å¹¶åˆ›å»ºtopics...")
    
    required_topics = ["otcol_logs", "otcol_metrics", "otcol_traces"]
    
    try:
        admin_client = create_admin_client()
        
        # è·å–ç°æœ‰topics
        existing_topics = admin_client.list_topics()
        print(f"  ç°æœ‰topics: {list(existing_topics)}")
        
        # æ£€æŸ¥å“ªäº›topicséœ€è¦åˆ›å»º
        topics_to_create = []
        for topic in required_topics:
            if topic in existing_topics:
                print(f"  âœ… Topic {topic} å·²å­˜åœ¨")
            else:
                print(f"  âš ï¸ Topic {topic} ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º")
                topics_to_create.append(topic)
        
        # åˆ›å»ºç¼ºå¤±çš„topics
        if topics_to_create:
            print(f"  ğŸ”¨ åˆ›å»ºtopics: {topics_to_create}")
            
            new_topics = [
                NewTopic(
                    name=topic,
                    num_partitions=6,  # 6ä¸ªåˆ†åŒº
                    replication_factor=1  # 1ä¸ªå‰¯æœ¬
                ) for topic in topics_to_create
            ]
            
            try:
                result = admin_client.create_topics(new_topics, validate_only=False)
                
                # ç­‰å¾…åˆ›å»ºå®Œæˆ
                try:
                    if hasattr(result, 'items'):
                        # å­—å…¸ç±»å‹
                        for topic, future in result.items():
                            try:
                                future.result()  # ç­‰å¾…ç»“æœ
                                print(f"  âœ… Topic {topic} åˆ›å»ºæˆåŠŸ")
                            except TopicAlreadyExistsError:
                                print(f"  âœ… Topic {topic} å·²å­˜åœ¨")
                            except Exception as e:
                                print(f"  âŒ Topic {topic} åˆ›å»ºå¤±è´¥: {str(e)}")
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œç›´æ¥æŠ¥å‘ŠæˆåŠŸ
                        for topic in topics_to_create:
                            print(f"  âœ… Topic {topic} åˆ›å»ºè¯·æ±‚å·²å‘é€")
                except Exception as e:
                    print(f"  âŒ å¤„ç†åˆ›å»ºç»“æœæ—¶å‡ºé”™: {str(e)}")
                        
            except Exception as e:
                print(f"  âŒ åˆ›å»ºtopicså¤±è´¥: {str(e)}")
        else:
            print("  âœ… æ‰€æœ‰å¿…éœ€çš„topicséƒ½å·²å­˜åœ¨")
        
        admin_client.close()
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥topicså¤±è´¥: {str(e)}")
        return False

def test_kafka_connection():
    """æµ‹è¯•Kafkaè¿æ¥"""
    print("ğŸ”— æµ‹è¯•Kafkaè¿æ¥...")
    
    try:
        ssl_context = create_ssl_context()
        
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BROKERS,
            security_protocol='SSL',
            ssl_context=ssl_context,
            value_serializer=lambda v: v.encode('utf-8'),
            client_id='local-test-producer',
            api_version=(2, 6, 0),
            request_timeout_ms=30000,
            retries=3
        )
        
        print("âœ… Kafkaè¿æ¥æˆåŠŸ")
        return producer
        
    except Exception as e:
        print(f"âŒ Kafkaè¿æ¥å¤±è´¥: {str(e)}")
        return None

def send_test_messages(producer):
    """å‘é€æµ‹è¯•æ¶ˆæ¯åˆ°3ä¸ªtopics"""
    print("\nğŸ“¤ å‘é€æµ‹è¯•æ¶ˆæ¯...")
    
    messages = {
        "otcol_logs": create_simple_log_message(),
        "otcol_metrics": create_simple_metrics_message(),
        "otcol_traces": create_simple_trace_message()
    }
    
    success_count = 0
    
    for topic, message in messages.items():
        try:
            print(f"  å‘é€åˆ° {topic}...")
            
            future = producer.send(topic, message)
            record_metadata = future.get(timeout=10)
            
            print(f"  âœ… {topic} å‘é€æˆåŠŸ (partition: {record_metadata.partition}, offset: {record_metadata.offset})")
            success_count += 1
            
        except Exception as e:
            print(f"  âŒ {topic} å‘é€å¤±è´¥: {str(e)}")
        
        time.sleep(1)
    
    return success_count

def verify_messages():
    """éªŒè¯æ¶ˆæ¯æ˜¯å¦å‘é€æˆåŠŸ"""
    print("\nğŸ” éªŒè¯æ¶ˆæ¯...")
    
    try:
        ssl_context = create_ssl_context()
        
        topics = ["otcol_logs", "otcol_metrics", "otcol_traces"]
        verified_count = 0
        
        for topic in topics:
            try:
                print(f"  æ£€æŸ¥ {topic}...")
                
                consumer = KafkaConsumer(
                    topic,
                    bootstrap_servers=KAFKA_BROKERS,
                    security_protocol='SSL',
                    ssl_context=ssl_context,
                    auto_offset_reset='latest',
                    consumer_timeout_ms=5000,
                    client_id=f'local-test-consumer-{topic}'
                )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†åŒºåˆ†é…
                partitions = consumer.partitions_for_topic(topic)
                if partitions:
                    print(f"    âœ… {topic} å­˜åœ¨ (åˆ†åŒº: {len(partitions)})")
                    verified_count += 1
                else:
                    print(f"    âš ï¸ {topic} ä¸å­˜åœ¨æˆ–æ— åˆ†åŒº")
                
                consumer.close()
                
            except Exception as e:
                print(f"    âŒ æ£€æŸ¥ {topic} å¤±è´¥: {str(e)}")
        
        return verified_count
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å¤±è´¥: {str(e)}")
        return 0

def main():
    print("ğŸš€ æœ¬åœ°Kafka mTLSæµ‹è¯•")
    print("=" * 40)
    print(f"è¿æ¥åˆ°Kafkaé›†ç¾¤: {KAFKA_BROKERS}")
    print("=" * 40)
    
    # æ£€æŸ¥å¹¶åˆ›å»ºtopics
    if not check_and_create_topics():
        print("âŒ Topicsæ£€æŸ¥/åˆ›å»ºå¤±è´¥ï¼Œä½†ç»§ç»­æµ‹è¯•è¿æ¥...")
    
    print("\n" + "=" * 40)
    
    # æµ‹è¯•è¿æ¥
    producer = test_kafka_connection()
    if not producer:
        print("âŒ æ— æ³•è¿æ¥åˆ°Kafkaï¼Œé€€å‡ºæµ‹è¯•")
        sys.exit(1)
    
    try:
        # å‘é€æ¶ˆæ¯
        success_count = send_test_messages(producer)
        
        # åˆ·æ–°å¹¶å…³é—­ç”Ÿäº§è€…
        producer.flush()
        producer.close()
        
        print(f"\nğŸ“Š å‘é€ç»“æœ: {success_count}/3 æˆåŠŸ")
        
        if success_count > 0:
            # éªŒè¯æ¶ˆæ¯
            verified_count = verify_messages()
            print(f"ğŸ“Š éªŒè¯ç»“æœ: {verified_count}/3 æˆåŠŸ")
            
            print("\n" + "=" * 40)
            print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
            print(f"  è¿æ¥çŠ¶æ€: âœ…")
            print(f"  æ¶ˆæ¯å‘é€: {success_count}/3")
            print(f"  TopicéªŒè¯: {verified_count}/3")
            
            if success_count > 0:
                print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼æ¶ˆæ¯å·²å‘é€åˆ°Kafkaé›†ç¾¤")
                print("ğŸ’¡ ç°åœ¨å¯ä»¥æ£€æŸ¥OpenTelemetry Collectoræ˜¯å¦æ¥æ”¶åˆ°æ¶ˆæ¯")
            else:
                print("\nâŒ æµ‹è¯•å¤±è´¥")
        else:
            print("\nâŒ æ²¡æœ‰æ¶ˆæ¯å‘é€æˆåŠŸ")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
    finally:
        if producer:
            producer.close()

if __name__ == "__main__":
    main() 