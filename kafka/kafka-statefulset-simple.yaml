apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: confluent-kafka
  labels:
    app: kafka
spec:
  serviceName: kafka-headless
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
      - name: kafka
        image: apache/kafka:latest
        ports:
        - containerPort: 9092
          name: kafka
        env:
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['apps.kubernetes.io/pod-index']
        - name: KAFKA_PROCESS_ROLES
          value: "broker,controller"
        - name: KAFKA_NODE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['apps.kubernetes.io/pod-index']
        - name: KAFKA_CONTROLLER_QUORUM_VOTERS
          value: "0@kafka-0.kafka-headless.confluent-kafka.svc.cluster.local:9093"
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://kafka-0.kafka-headless.confluent-kafka.svc.cluster.local:9092"
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT"
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: "CONTROLLER"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "PLAINTEXT"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "1"
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "1"
        - name: KAFKA_LOG_DIRS
          value: "/var/lib/kafka/data/kafka-logs"
        - name: CLUSTER_ID
          value: "MkU3OEVBNTcwNTJENDM2Qk"
        
        command:
        - sh
        - -c
        - |
          # 清理lost+found目录（如果存在）
          if [ -d "/var/lib/kafka/data/lost+found" ]; then
            echo "删除lost+found目录..."
            rm -rf /var/lib/kafka/data/lost+found
          fi
          
          # 确保Kafka数据目录存在
          mkdir -p /var/lib/kafka/data/kafka-logs
          
          # 创建临时配置文件
          cat > /tmp/server.properties << EOF
          # Basic Kafka configuration
          process.roles=broker,controller
          node.id=${KAFKA_NODE_ID}
          controller.quorum.voters=${KAFKA_CONTROLLER_QUORUM_VOTERS}
          
          # Listener configuration
          listeners=${KAFKA_LISTENERS}
          advertised.listeners=${KAFKA_ADVERTISED_LISTENERS}
          listener.security.protocol.map=${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
          controller.listener.names=${KAFKA_CONTROLLER_LISTENER_NAMES}
          inter.broker.listener.name=${KAFKA_INTER_BROKER_LISTENER_NAME}
          
          # Log configuration - 使用子目录避免lost+found问题
          log.dirs=/var/lib/kafka/data/kafka-logs
          num.network.threads=3
          num.io.threads=8
          socket.send.buffer.bytes=102400
          socket.receive.buffer.bytes=102400
          socket.request.max.bytes=104857600
          
          # Replication settings
          offsets.topic.replication.factor=${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
          transaction.state.log.replication.factor=${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}
          transaction.state.log.min.isr=${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}
          
          # Other settings
          num.partitions=1
          num.recovery.threads.per.data.dir=1
          log.retention.hours=168
          log.segment.bytes=1073741824
          log.retention.check.interval.ms=300000
          EOF
          
          # 初始化KRaft存储
          if [ ! -f "/var/lib/kafka/data/.format_done" ]; then
            echo "格式化Kafka存储..."
            /opt/kafka/bin/kafka-storage.sh format -t ${CLUSTER_ID} -c /tmp/server.properties
            touch /var/lib/kafka/data/.format_done
          fi
          
          # 启动Kafka
          exec /opt/kafka/bin/kafka-server-start.sh /tmp/server.properties
        
        volumeMounts:
        - name: kafka-data
          mountPath: /var/lib/kafka/data
        
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 1000m
            memory: 4Gi
        
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 10
          
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 60
          periodSeconds: 30
  
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ssd
      resources:
        requests:
          storage: 10Gi 